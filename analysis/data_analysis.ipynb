{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76286865",
   "metadata": {},
   "source": [
    "# Statistical Analysis: NHST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab0a393",
   "metadata": {},
   "source": [
    "## Data loading & pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ff394e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import analysis_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eba136",
   "metadata": {},
   "source": [
    "Load data from all `../res` sub-directories (treatments):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d992f20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dfs = {}    # dict to store statistical summary data (<session_name>.json)\n",
    "all_data_dfs = {}   # dict to store all individual measurements (all_data_<session_name>.json)\n",
    "\n",
    "experiment_data_dir = '../res' # directory generated by \"eval.py\", contains the .json files with the experiment data\n",
    "\n",
    "iteration_structure = True # flag to set the directory structure of the input data\n",
    "\n",
    "# Load all the experiment data\n",
    "summary_dfs, all_data_dfs = utils.load_experiment_data(experiment_data_dir, iteration_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d62f67",
   "metadata": {},
   "source": [
    "### Flatten GPU and VRAM columns \n",
    "These contains nested objects and values.\n",
    "We are only interested in a subset of the contained data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "392db5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_gpu_vram_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # TODO: fix the key for extracting \"vram_max_usage_mib\" from \"max_consumed_MiB\" -> new format (check repo)\n",
    "    \n",
    "    # GPU columns\n",
    "    df[\"gpu_util_mean\"] = df[\"GPU\"].apply(lambda x: x[\"utilization\"][\"avg\"] if isinstance(x, dict) else None)\n",
    "    df[\"gpu_util_max\"]  = df[\"GPU\"].apply(lambda x: x[\"utilization\"][\"max\"] if isinstance(x, dict) else None)\n",
    "\n",
    "    # VRAM columns\n",
    "    df[\"vram_util_mean\"]     = df[\"VRAM\"].apply(lambda x: x[\"utilization\"][\"avg\"] if isinstance(x, dict) else None)\n",
    "    df[\"vram_util_max\"]      = df[\"VRAM\"].apply(lambda x: x[\"utilization\"][\"max\"] if isinstance(x, dict) else None)\n",
    "    df[\"vram_max_usage_mib\"] = df[\"VRAM\"].apply(lambda x: x[\"max_consumed_MiB\"] if isinstance(x, dict) else None) # TODO: FIX KEY\n",
    "\n",
    "    # Drop the redundant columns\n",
    "    df = df.drop(columns=[\"GPU\", \"VRAM\"])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c288f3c",
   "metadata": {},
   "source": [
    "### Convert data to long format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7737e465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_dfs_to_long(dictionary_dataframes: dict[pd.DataFrame], metrics_of_interest: list[str]) -> pd.DataFrame:\n",
    "    rows = [] # Temporary list to hold all constructed rows of data\n",
    "\n",
    "    # Iterate through each treatment and corresponding data\n",
    "    for treatment_name, df in dictionary_dataframes.items():\n",
    "\n",
    "        # Extract the treatment factors (dataset is a blocked variable)\n",
    "        model, quant, dataset = treatment_name.split(\"_\")\n",
    "        \n",
    "        for i, row in df.iterrows():\n",
    "            # Extract iteration from last dir in data_path (e.g. \"01\" from \".../01\", etc.)\n",
    "            match = re.search(r\"/(\\d{2})$\", row[\"data_path\"])\n",
    "            iteration = int(match.group(1)) if match else i + 1 # TODO: should we include the fallback or let it fail?\n",
    "\n",
    "            # Construct row dictionary\n",
    "            row_data = {\n",
    "                \"treatment\": treatment_name,\n",
    "                \"model\": model, \n",
    "                \"quantization\": quant, \n",
    "                \"dataset\": dataset,\n",
    "                \"iteration\": iteration\n",
    "            }\n",
    "        \n",
    "            # Add relevant metrics\n",
    "            for metric in metrics_of_interest:\n",
    "                row_data[metric] = row[metric]\n",
    "\n",
    "            rows.append(row_data)\n",
    "\n",
    "    # Combine all the rows of data into a single long-format dataframe\n",
    "    long_dataframe: pd.DataFrame = pd.DataFrame(rows)\n",
    "\n",
    "    return long_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e09809",
   "metadata": {},
   "source": [
    "### Flatten columns and convert to long format to prepare for NHST analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ba74a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      treatment model quantization dataset  iteration  accuracy  recall  \\\n",
      "0  MIS_AWQ_BTHS   MIS          AWQ    BTHS          1  0.850000    0.10   \n",
      "1  MIS_AWQ_BTHS   MIS          AWQ    BTHS          2  0.850000    0.10   \n",
      "2  MIS_AWQ_BTHS   MIS          AWQ    BTHS          3  0.850000    0.10   \n",
      "3  MIS_AWQ_BTHS   MIS          AWQ    BTHS          4  0.841667    0.05   \n",
      "4  MIS_AWQ_BTHS   MIS          AWQ    BTHS          5  0.841667    0.05   \n",
      "\n",
      "   precision        f1  balanced_accuracy  specificity  \n",
      "0        1.0  0.181818           0.923729     0.847458  \n",
      "1        1.0  0.181818           0.923729     0.847458  \n",
      "2        1.0  0.181818           0.923729     0.847458  \n",
      "3        1.0  0.095238           0.920168     0.840336  \n",
      "4        1.0  0.095238           0.920168     0.840336  \n"
     ]
    }
   ],
   "source": [
    "efficacy_metrics = [\n",
    "    \"accuracy\", \"recall\", \n",
    "    \"precision\", \"f1\",\n",
    "    \"balanced_accuracy\", \n",
    "    \"specificity\"\n",
    "]\n",
    "\n",
    "efficiency_metrics = [\n",
    "    \"time_to_analyze\",\n",
    "    \"gpu_util_mean\", \"gpu_util_max\",\n",
    "    \"vram_util_mean\", \"vram_util_max\",\n",
    "    \"vram_max_usage_mib\"\n",
    "]\n",
    "\n",
    "# Flatten and extract the relevant GPU and VRAM metrics into distinct columns (they are intially nested objects)\n",
    "flattened_dfs = {key: flatten_gpu_vram_columns(df) for key, df in all_data_dfs.items()}\n",
    "\n",
    "# Convert data to long format; group by research question\n",
    "long_df_efficacy = convert_dict_dfs_to_long(flattened_dfs, efficacy_metrics)\n",
    "long_df_efficiency = convert_dict_dfs_to_long(flattened_dfs, efficiency_metrics)\n",
    "\n",
    "# Sort the dataframes using custom ordering schemes\n",
    "# Note: we can apply custom schemes to all the sorting columns if needed\n",
    "sorting_order = [\"dataset\", \"iteration\", \"model\", \"quantization\"]\n",
    "quant_order = [\"NONE\", \"AWQ\", \"GPTQ\", \"AQLM\"]\n",
    "#long_df_efficacy[\"quantization\"] = pd.Categorical(long_df_efficacy[\"quantization\"], categories=quant_order, ordered=True)\n",
    "#long_df_efficacy = long_df_efficacy.sort_values(by=sorting_order).reset_index(drop=True)\n",
    "\n",
    "print(long_df_efficacy.head())\n",
    "#print(long_df_efficiency.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ceb859",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eedff3",
   "metadata": {},
   "source": [
    "## Testing ANOVA assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994b4338",
   "metadata": {},
   "source": [
    "#### Big questions:\n",
    "\n",
    "- do we use repeated-measures (RM) ANOVA or RM MANOVA (multivariate ANOVA)?\n",
    "  - this choice depends on how we want to treat the DVs (dependent variables);\n",
    "  - i.e., do we group efficacy metrics since they are interconnected?\n",
    "- alternatively, what about using the Friedman test?\n",
    "\n",
    "\n",
    "Option:\n",
    "- MANOVA for RQ1: the confusion matrix metrics are interconnected\n",
    "- ANOVA for RQ2: these metrics are independent\n",
    "  - we simplify to just: time, max vram usage\n",
    "\n",
    "Or: \n",
    "- Repeated Measures ANOVA for RQ1\n",
    "- ANOVA for RQ2: same simplified dependent variables\n",
    "\n",
    "Or:\n",
    "- Friedman test for RQ1\n",
    "- Friedman test for RQ2 with simplified DVs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95340cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import shapiro, levene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815824b6",
   "metadata": {},
   "source": [
    "### Univariate ANOVA assumption checks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a589c91c",
   "metadata": {},
   "source": [
    "Shapiro-Wilks test (normality assumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "234905a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_anova_normality(df: pd.DataFrame, metrics: list[str], print_results = True) -> dict:\n",
    "    results = {} # Shapiro test results\n",
    "    alpha = 0.05 # Significance level\n",
    "\n",
    "    # Check each metric individually\n",
    "    for metric in metrics:\n",
    "        results[metric] = []\n",
    "\n",
    "        if print_results: print(f\"\\nChecking assumptions for {metric.upper()}:\")\n",
    "\n",
    "        # Group by treatment\n",
    "        for treatment in df[\"treatment\"].unique():\n",
    "            subset = df[df[\"treatment\"] == treatment][metric]\n",
    "            \n",
    "            # Shapiro-Wilk test\n",
    "            w, p = shapiro(subset)\n",
    "            results[metric].append({treatment: [{\"W\": w, \"p_value\": p}]})\n",
    "\n",
    "            # Print the result\n",
    "            if print_results: \n",
    "                print(f\"  Treatment: {treatment}\")\n",
    "                print(f\"  Shapiro-Wilk p = {p:.4f} --> {'Pass' if p > alpha else 'Fail'} (normality)\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17bb407",
   "metadata": {},
   "source": [
    "Check normality for RQ1: efficacy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03136170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking assumptions for ACCURACY:\n",
      "  Treatment: MIS_AWQ_BTHS\n",
      "  Shapiro-Wilk p = 0.0000 --> Fail (normality)\n",
      "  Treatment: MIS_AWQ_ENCO\n",
      "  Shapiro-Wilk p = 0.0021 --> Fail (normality)\n",
      "  Treatment: MIS_AWQ_SNAKE\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n",
      "  Treatment: MIS_NONE_BTHS\n",
      "  Shapiro-Wilk p = 0.0000 --> Fail (normality)\n",
      "  Treatment: MIS_NONE_ENCO\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n",
      "  Treatment: MIS_NONE_SNAKE\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n",
      "\n",
      "Checking assumptions for RECALL:\n",
      "  Treatment: MIS_AWQ_BTHS\n",
      "  Shapiro-Wilk p = 0.0000 --> Fail (normality)\n",
      "  Treatment: MIS_AWQ_ENCO\n",
      "  Shapiro-Wilk p = 0.0002 --> Fail (normality)\n",
      "  Treatment: MIS_AWQ_SNAKE\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n",
      "  Treatment: MIS_NONE_BTHS\n",
      "  Shapiro-Wilk p = 0.0000 --> Fail (normality)\n",
      "  Treatment: MIS_NONE_ENCO\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n",
      "  Treatment: MIS_NONE_SNAKE\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n",
      "\n",
      "Checking assumptions for PRECISION:\n",
      "  Treatment: MIS_AWQ_BTHS\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n",
      "  Treatment: MIS_AWQ_ENCO\n",
      "  Shapiro-Wilk p = 0.0000 --> Fail (normality)\n",
      "  Treatment: MIS_AWQ_SNAKE\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n",
      "  Treatment: MIS_NONE_BTHS\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n",
      "  Treatment: MIS_NONE_ENCO\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n",
      "  Treatment: MIS_NONE_SNAKE\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n",
      "\n",
      "Checking assumptions for F1:\n",
      "  Treatment: MIS_AWQ_BTHS\n",
      "  Shapiro-Wilk p = 0.0000 --> Fail (normality)\n",
      "  Treatment: MIS_AWQ_ENCO\n",
      "  Shapiro-Wilk p = 0.0022 --> Fail (normality)\n",
      "  Treatment: MIS_AWQ_SNAKE\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n",
      "  Treatment: MIS_NONE_BTHS\n",
      "  Shapiro-Wilk p = 0.0000 --> Fail (normality)\n",
      "  Treatment: MIS_NONE_ENCO\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n",
      "  Treatment: MIS_NONE_SNAKE\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n",
      "\n",
      "Checking assumptions for BALANCED_ACCURACY:\n",
      "  Treatment: MIS_AWQ_BTHS\n",
      "  Shapiro-Wilk p = 0.0000 --> Fail (normality)\n",
      "  Treatment: MIS_AWQ_ENCO\n",
      "  Shapiro-Wilk p = 0.0000 --> Fail (normality)\n",
      "  Treatment: MIS_AWQ_SNAKE\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n",
      "  Treatment: MIS_NONE_BTHS\n",
      "  Shapiro-Wilk p = 0.0000 --> Fail (normality)\n",
      "  Treatment: MIS_NONE_ENCO\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n",
      "  Treatment: MIS_NONE_SNAKE\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n",
      "\n",
      "Checking assumptions for SPECIFICITY:\n",
      "  Treatment: MIS_AWQ_BTHS\n",
      "  Shapiro-Wilk p = 0.0000 --> Fail (normality)\n",
      "  Treatment: MIS_AWQ_ENCO\n",
      "  Shapiro-Wilk p = 0.0002 --> Fail (normality)\n",
      "  Treatment: MIS_AWQ_SNAKE\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n",
      "  Treatment: MIS_NONE_BTHS\n",
      "  Shapiro-Wilk p = 0.0000 --> Fail (normality)\n",
      "  Treatment: MIS_NONE_ENCO\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n",
      "  Treatment: MIS_NONE_SNAKE\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Erik\\Documents\\University\\Year-3\\BSc-Thesis\\Repositories\\Upstream REST-at\\.venv\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:586: UserWarning: scipy.stats.shapiro: Input data has range zero. The results may not be accurate.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    }
   ],
   "source": [
    "_ = check_anova_normality(long_df_efficacy, efficacy_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bfc63a",
   "metadata": {},
   "source": [
    "Check normality for RQ2: efficiency metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6bfe011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking assumptions for TIME_TO_ANALYZE:\n",
      "  Treatment: MIS_AWQ_BTHS\n",
      "  Shapiro-Wilk p = 0.0164 --> Fail (normality)\n",
      "  Treatment: MIS_AWQ_ENCO\n",
      "  Shapiro-Wilk p = 0.8446 --> Pass (normality)\n",
      "  Treatment: MIS_AWQ_SNAKE\n",
      "  Shapiro-Wilk p = 0.7515 --> Pass (normality)\n",
      "  Treatment: MIS_NONE_BTHS\n",
      "  Shapiro-Wilk p = 0.0461 --> Fail (normality)\n",
      "  Treatment: MIS_NONE_ENCO\n",
      "  Shapiro-Wilk p = 0.5152 --> Pass (normality)\n",
      "  Treatment: MIS_NONE_SNAKE\n",
      "  Shapiro-Wilk p = 0.0689 --> Pass (normality)\n",
      "\n",
      "Checking assumptions for VRAM_MAX_USAGE_MIB:\n",
      "  Treatment: MIS_AWQ_BTHS\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n",
      "  Treatment: MIS_AWQ_ENCO\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n",
      "  Treatment: MIS_AWQ_SNAKE\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n",
      "  Treatment: MIS_NONE_BTHS\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n",
      "  Treatment: MIS_NONE_ENCO\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n",
      "  Treatment: MIS_NONE_SNAKE\n",
      "  Shapiro-Wilk p = 1.0000 --> Pass (normality)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Erik\\Documents\\University\\Year-3\\BSc-Thesis\\Repositories\\Upstream REST-at\\.venv\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:586: UserWarning: scipy.stats.shapiro: Input data has range zero. The results may not be accurate.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    }
   ],
   "source": [
    "simple_efficiency_metrics = [\n",
    "    \"time_to_analyze\",\n",
    "    \"vram_max_usage_mib\"\n",
    "]\n",
    "\n",
    "_ = check_anova_normality(long_df_efficiency, simple_efficiency_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Enhanced REST-at)",
   "language": "python",
   "name": "my-new-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
