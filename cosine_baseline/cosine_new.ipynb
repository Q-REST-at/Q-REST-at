{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_process(req_path, test_path):\n",
    "    req_df = pd.read_csv(req_path, sep=',', on_bad_lines='skip')\n",
    "    test_df = pd.read_csv(test_path, sep=',', on_bad_lines='skip')\n",
    "    \n",
    "    # Some Purpose columns are intentionally left blank for now; populate them with empty strings\n",
    "    test_df['Purpose'] = test_df['Purpose'].fillna('')\n",
    "\n",
    "    req_text_fields = ['Feature', 'Description']\n",
    "    test_text_fields = ['Purpose', 'Test steps']\n",
    "\n",
    "    # Combine text columns for similarity matching\n",
    "    req_df['full_text'] = req_df[req_text_fields].astype(str).agg(' '.join, axis=1)\n",
    "    test_df['full_text'] = test_df[test_text_fields].astype(str).agg(' '.join, axis=1)\n",
    "    \n",
    "    # Convert to lists\n",
    "    req_texts = req_df['full_text'].tolist()\n",
    "    test_texts = test_df['full_text'].tolist()\n",
    "\n",
    "    return req_df, test_df, req_texts, test_texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(pred_dict, ground_truth_path, test_path, debug=False):\n",
    "    \n",
    "    gt_df = pd.read_csv(ground_truth_path).dropna(subset=['Req ID'])\n",
    "    gt_dict = {\n",
    "        row['Req ID']: list(map(str.strip, str(row['Test ID']).split(','))) if pd.notna(row['Test ID']) else []\n",
    "        for _, row in gt_df.iterrows()\n",
    "    }\n",
    "\n",
    "    ### CONFUSION MATRIX CALCULATION ###\n",
    "    \n",
    "    # Values for confusion matrix\n",
    "    n: int = 0\n",
    "    tp: int = 0\n",
    "    tn: int = 0\n",
    "    fp: int = 0\n",
    "    fn: int = 0\n",
    "\n",
    "    test_test = pd.read_csv(test_path, sep=',', on_bad_lines='skip')\n",
    "    test_text_fields = ['ID', 'Purpose', 'Test steps']\n",
    "    test_test['full_text'] = test_test[test_text_fields].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "    curr_tests: set[str]\n",
    "    curr_tests: set[str] = set(test_test[\"ID\"].astype(str))\n",
    "\n",
    "    frequency_table: dict[bool, dict[str, dict[str, int]]] = {True: {}, False: {}}\n",
    "\n",
    "    for req in set(pred_dict.keys()) | set(gt_dict.keys()):\n",
    "        actual_tests: set[str] = set(pred_dict.get(req, []))\n",
    "        expected_tests: set[str] = set(gt_dict.get(req, []))\n",
    "\n",
    "        # Skip if req ID returned None\n",
    "        if expected_tests is None:\n",
    "            print(f\"Error - {current_dir}: Faulty requirement ID ({req})\")\n",
    "            continue\n",
    "\n",
    "        # Positives\n",
    "        curr_tp_set: set[str] = actual_tests & expected_tests\n",
    "        curr_tp_count: int = len(curr_tp_set)\n",
    "        if debug:\n",
    "            print(f\"Info - \\t\\t({curr_tp_count}) {curr_tp_set = }\")\n",
    "\n",
    "        curr_fp_set: set[str] = actual_tests - expected_tests\n",
    "        curr_fp_count: int = len(curr_fp_set)\n",
    "        if debug:\n",
    "            print(f\"Info - \\t\\t({curr_fp_count}) {curr_fp_set = }\")\n",
    "        \n",
    "        # Negatives\n",
    "        expected_ns: set[str] = curr_tests - expected_tests\n",
    "        actual_ns: set[str] = curr_tests - actual_tests\n",
    "\n",
    "        curr_tn_set: set[str] = actual_ns & expected_ns\n",
    "        curr_tn_count: int = len(curr_tn_set)\n",
    "        if debug:\n",
    "            print(f\"Info - \\t\\t({curr_tn_count}) {curr_tn_set = }\")\n",
    "\n",
    "        curr_fn_set: set[str] = actual_ns - expected_ns\n",
    "        curr_fn_count: int = len(curr_fn_set)\n",
    "        if debug:\n",
    "            print(f\"Info - \\t\\t({curr_fn_count}) {curr_fn_set = }\")\n",
    "\n",
    "        curr_n: int = curr_tp_count + curr_fp_count + curr_tn_count + curr_fn_count\n",
    "        \n",
    "        # Check so only the right amount of trace links were detected\n",
    "        expected_curr_n: int = len(curr_tests)\n",
    "        if curr_n != expected_curr_n:\n",
    "            print(f\"Error - \\t\\tExpected curr_n = {expected_curr_n}, got {curr_n = }\")\n",
    "        if debug:\n",
    "            print(f\"Info - \\t\\t{curr_n = }\")\n",
    "\n",
    "        # Update the frequency table\n",
    "\n",
    "        # Get the true positives\n",
    "        true_positives: dict[str, int] = frequency_table[True].get(req, None)\n",
    "        # Assign a dict if one doesn't exist\n",
    "        if true_positives is None:\n",
    "            true_positives = {}\n",
    "            frequency_table[True][req] = true_positives\n",
    "        \n",
    "        # Get the false positives\n",
    "        false_positives: dict[str, int] = frequency_table[False].get(req, None)\n",
    "        # Assign a dict if one doesn't exist\n",
    "        if false_positives is None:\n",
    "            false_positives = {}\n",
    "            frequency_table[False][req] = false_positives\n",
    "\n",
    "        # Add 1 for each true positive link\n",
    "        for test in curr_tp_set:\n",
    "            true_positives[test] = true_positives.get(test, 0) + 1\n",
    "\n",
    "        # Add 1 for each false positive link\n",
    "        for test in curr_fp_set:\n",
    "            false_positives[test] = false_positives.get(test, 0) + 1\n",
    "\n",
    "\n",
    "        n += curr_n\n",
    "        tp += curr_tp_count\n",
    "        tn += curr_tn_count\n",
    "        fp += curr_fp_count\n",
    "        fn += curr_fn_count\n",
    "    \n",
    "    accuracy: float = (tp + tn) / n if n != 0 else 0.0\n",
    "    recall: float = tp / (tp + fn) if tp + fn != 0 else 0.0\n",
    "    precision: float = tp / (tp + fp) if tp + fp != 0 else 0.0\n",
    "    specificity: float = tn / (tn + fn) if tn + fn != 0 else 0.0\n",
    "    balanced_accuracy: float = (precision + specificity) / 2\n",
    "    f1: float = 2 * (recall * precision) / (recall + precision) if recall + precision != 0 else 0.0\n",
    "\n",
    "    if debug:\n",
    "        print(\"*********************************************\")\n",
    "        print(ground_truth_path)\n",
    "\n",
    "    if debug:\n",
    "        print(\"*********************************************\")\n",
    "        print(f\"Accuracy: {balanced_accuracy:.2f}%\")\n",
    "        print(\"*********************************************\")\n",
    "\n",
    "  \n",
    "    # assert tp + fn == 134\n",
    "\n",
    "    return tp, fp, tn, fn, accuracy, recall, precision, balanced_accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def custom_preprocessor(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "\n",
    "def compute_cosine(req_df, test_df, req_texts, test_texts, threshold):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        preprocessor=custom_preprocessor,\n",
    "        tokenizer=custom_tokenizer,\n",
    "        token_pattern=None\n",
    "    )\n",
    "\n",
    "    documents = req_texts + test_texts\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix[:len(req_texts)], tfidf_matrix[len(req_texts):])\n",
    "\n",
    "    matches = {}\n",
    "    for i, req_id in enumerate(req_df['ID']):\n",
    "        matched_test_ids = []\n",
    "        for j, similarity in enumerate(similarity_matrix[i]):\n",
    "            if similarity >= threshold:\n",
    "                matched_test_ids.append(str(test_df.iloc[j]['ID']))\n",
    "        matches[str(req_id)] = matched_test_ids\n",
    "        \n",
    "    print(f\"Matches for threshold {threshold:.2f}: {len(matches)}\")\n",
    "    print(f\"Matches: {matches}\")\n",
    "\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logic including subsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: HealthWatcher. Subset: 01\n",
      "Threshold: 0.34\n",
      "Matches for threshold 0.34: 9\n",
      "Matches: {'FR01': ['T-1'], 'FR02': ['T-5'], 'FR10': [], 'FR11': ['T-1'], 'FR12': ['T-2', 'T-5'], 'FR13': ['T-6'], 'FR14': ['T-6', 'T-7'], 'FR15': ['T-1', 'T-8'], 'FR16': []}\n",
      "*********************************************\n",
      "Dataset: HealthWatcher. Subset: 02\n",
      "Threshold: 0.34\n",
      "Matches for threshold 0.34: 9\n",
      "Matches: {'FR01': ['T-1'], 'FR02': ['T-5'], 'FR10': [], 'FR11': ['T-1'], 'FR12': ['T-2', 'T-5'], 'FR13': ['T-6'], 'FR14': ['T-6', 'T-7'], 'FR15': ['T-1', 'T-8'], 'FR16': []}\n",
      "*********************************************\n",
      "Dataset: HealthWatcher. Subset: 03\n",
      "Threshold: 0.34\n",
      "Matches for threshold 0.34: 9\n",
      "Matches: {'FR01': ['T-1'], 'FR02': ['T-5'], 'FR10': [], 'FR11': ['T-1'], 'FR12': ['T-2', 'T-5'], 'FR13': ['T-6'], 'FR14': ['T-6', 'T-7'], 'FR15': ['T-1', 'T-8'], 'FR16': []}\n",
      "*********************************************\n",
      "Dataset: HealthWatcher. Subset: 04\n",
      "Threshold: 0.34\n",
      "Matches for threshold 0.34: 9\n",
      "Matches: {'FR01': ['T-1'], 'FR02': ['T-5'], 'FR10': [], 'FR11': ['T-1'], 'FR12': ['T-2', 'T-5'], 'FR13': ['T-6'], 'FR14': ['T-6', 'T-7'], 'FR15': ['T-1', 'T-8'], 'FR16': []}\n",
      "*********************************************\n",
      "Dataset: HealthWatcher. Subset: 05\n",
      "Threshold: 0.34\n",
      "Matches for threshold 0.34: 9\n",
      "Matches: {'FR01': ['T-1'], 'FR02': ['T-5'], 'FR10': [], 'FR11': ['T-1'], 'FR12': ['T-2', 'T-5'], 'FR13': ['T-6'], 'FR14': ['T-6', 'T-7'], 'FR15': ['T-1', 'T-8'], 'FR16': []}\n",
      "*********************************************\n",
      "Dataset: HealthWatcher. Subset: 06\n",
      "Threshold: 0.34\n",
      "Matches for threshold 0.34: 9\n",
      "Matches: {'FR01': ['T-1'], 'FR02': ['T-5'], 'FR10': [], 'FR11': ['T-1'], 'FR12': ['T-2', 'T-5'], 'FR13': ['T-6'], 'FR14': ['T-6', 'T-7'], 'FR15': ['T-1', 'T-8'], 'FR16': []}\n",
      "*********************************************\n",
      "Dataset: HealthWatcher. Subset: 07\n",
      "Threshold: 0.34\n",
      "Matches for threshold 0.34: 9\n",
      "Matches: {'FR01': ['T-1'], 'FR02': ['T-5'], 'FR10': [], 'FR11': ['T-1'], 'FR12': ['T-2', 'T-5'], 'FR13': ['T-6'], 'FR14': ['T-6', 'T-7'], 'FR15': ['T-1', 'T-8'], 'FR16': []}\n",
      "*********************************************\n",
      "Dataset: HealthWatcher. Subset: 08\n",
      "Threshold: 0.34\n",
      "Matches for threshold 0.34: 9\n",
      "Matches: {'FR01': ['T-1'], 'FR02': ['T-5'], 'FR10': [], 'FR11': ['T-1'], 'FR12': ['T-2', 'T-5'], 'FR13': ['T-6'], 'FR14': ['T-6', 'T-7'], 'FR15': ['T-1', 'T-8'], 'FR16': []}\n",
      "*********************************************\n",
      "Dataset: HealthWatcher. Subset: 09\n",
      "Threshold: 0.34\n",
      "Matches for threshold 0.34: 9\n",
      "Matches: {'FR01': ['T-1'], 'FR02': ['T-5'], 'FR10': [], 'FR11': ['T-1'], 'FR12': ['T-2', 'T-5'], 'FR13': ['T-6'], 'FR14': ['T-6', 'T-7'], 'FR15': ['T-1', 'T-8'], 'FR16': []}\n",
      "*********************************************\n",
      "Dataset: HealthWatcher. Subset: 10\n",
      "Threshold: 0.34\n",
      "Matches for threshold 0.34: 9\n",
      "Matches: {'FR01': ['T-1'], 'FR02': ['T-5'], 'FR10': [], 'FR11': ['T-1'], 'FR12': ['T-2', 'T-5'], 'FR13': ['T-6'], 'FR14': ['T-6', 'T-7'], 'FR15': ['T-1', 'T-8'], 'FR16': []}\n",
      "*********************************************\n",
      "\n",
      "\n",
      " Threshold  F1 score   Recall  Precision\n",
      "      0.34  0.526316 0.555556        0.5\n",
      "üèÜ Best global threshold: 0.34 with average F1 score: 0.5263\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Datasets folders \n",
    "datasets = [\"HealthWatcher\"]\n",
    "subsets = [f\"{i:02}\" for i in range(1, 11)]  # 01 to 10\n",
    "summary_table = pd.DataFrame()\n",
    "thresholds = [0.34]\n",
    "# thresholds = np.arange(0.01, 1.01, 0.01)\n",
    "\n",
    "# For every dataset find the following:\n",
    "for dataset in datasets:    \n",
    "    for subset in subsets:\n",
    "        req_path = f'../data/{dataset}/{subset}/RE.csv'\n",
    "        test_path = f'../data/{dataset}/{subset}/ST.csv'\n",
    "        ground_truth_mapping_path = f'../data/{dataset}/{subset}/mapping.csv'\n",
    "        \n",
    "        print(f\"Dataset: {dataset}. Subset: {subset}\")\n",
    "        \n",
    "        # Load and process data\n",
    "        req_df, test_df, req_texts, test_texts = load_and_process(req_path, test_path)\n",
    "        \n",
    "        # Find cosine similarity and measure execution time\n",
    "        start = time.time()\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            threshold_str = f\"{threshold:.2f}\"\n",
    "            print(f\"Threshold: {threshold:.2f}\")\n",
    "            predicted_matches = compute_cosine(req_df, test_df, req_texts, test_texts, threshold)\n",
    "            t = time.time() - start\n",
    "            tp, fp, tn, fn, accuracy, recall, precision, balanced_accuracy, f1 = calculate_accuracy(predicted_matches, ground_truth_mapping_path, test_path, debug=False)\n",
    "            summary_table = pd.concat([\n",
    "                summary_table,\n",
    "                pd.DataFrame([{\"Dataset\": dataset, \"Subset\": subset, \"Threshold\": f\"{threshold:.2f}\", \"TP\": tp, \"FP\": fp, \"TN\": tn, \"FN\": fn, \"Accuracy\": accuracy, \"Recall\": recall, \"Precision\": precision, \"Balanced Accuracy\": balanced_accuracy, \"F1 score\": f1}])\n",
    "            ], ignore_index=True)\n",
    "            print(\"*********************************************\")\n",
    "\n",
    "        \n",
    "            payload: dict[str, dict] = {\n",
    "                \"meta\": {\n",
    "                    \"req_path\": req_path,\n",
    "                    \"test_path\": test_path,\n",
    "                    \"mapping_path\": ground_truth_mapping_path\n",
    "                },\n",
    "                \"data\": {\n",
    "                    \"links\": predicted_matches,\n",
    "                    \"time-to-analyze\": t\n",
    "                }\n",
    "            }\n",
    "            current_time = time.strftime(\"%H_%M_%S\", time.localtime())\n",
    "            current_date = time.strftime(\"%Y-%m-%d\", time.localtime())\n",
    "\n",
    "            log_dir: str = f\"./MIS_COSINE_{dataset}/{current_date}/{current_time}/{subset}\"\n",
    "            os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "            with open(f\"{log_dir}/res.json\", \"w+\") as out:\n",
    "                json.dump(payload, out, indent=2)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Convert Threshold back to float for sorting if necessary\n",
    "summary_table[\"Threshold\"] = summary_table[\"Threshold\"].astype(float)\n",
    "summary_table = summary_table.sort_values(by=[\"Threshold\", \"Dataset\", \"Subset\"])\n",
    "grouped = summary_table.groupby([\"Threshold\"], as_index=False).agg({\"F1 score\": \"mean\", \"Recall\": \"mean\", \"Precision\": \"mean\"})\n",
    "grouped = grouped.sort_values(by=[\"F1 score\", \"Recall\", \"Precision\"], ascending=[False, False, False])\n",
    "\n",
    "# avg_f1_per_threshold = grouped.groupby(\"Threshold\", as_index=False).agg({\"F1 score\": \"mean\"})\n",
    "best_row = grouped.loc[grouped[\"F1 score\"].idxmax()]\n",
    "\n",
    "# Print Best Metrics per Subset (based on max F1 score)\n",
    "print(\"\\nBest Metrics per Subset (Based on F1 Score):\")\n",
    "print(\"+------------------+----------+----------------+----------+---------+----------+---------+------------+--------+----------+-------------------+-------------------+\")\n",
    "print(\"| Dataset          | Subset   | Threshold      | TP       |FP       | TN       |FN       |Accuracy    | Recall | Precision| Balanced Accuracy |      F1 score     |\")\n",
    "print(\"+------------------+----------+----------------+----------+---------+----------+---------+------------+--------+----------+-------------------+-------------------+\")\n",
    "for _, row in summary_table.iterrows():\n",
    "    print(\n",
    "        f\"| {row['Dataset']:<16} \"\n",
    "        f\"| {row['Subset']:<8} \"\n",
    "        f\"| {row['Threshold']:<14} \"\n",
    "        f\"| {int(row['TP']):<8} \"\n",
    "        f\"| {int(row['FP']):<7} \"\n",
    "        f\"| {int(row['TN']):<8} \"\n",
    "        f\"| {int(row['FN']):<7} \"\n",
    "        f\"| {row['Accuracy']:<10.2f} \"\n",
    "        f\"| {row['Recall']:<6.2f} \"\n",
    "        f\"| {row['Precision']:<8.2f} \"\n",
    "        f\"| {row['Balanced Accuracy']:<18f}\"\n",
    "        f\"| {row['F1 score']:<18f}|\")\n",
    "print(\"+----------------------+------------------+----------------+----------+---------+----------+---------+------------+--------+----------+-------------------+-------------------+\")\n",
    "\n",
    "print(grouped.to_string(index=False))\n",
    "\n",
    "best_threshold = best_row[\"Threshold\"]\n",
    "best_avg_f1 = best_row[\"F1 score\"]\n",
    "print(f\"üèÜ Best global threshold: {best_threshold:.2f} with average F1 score: {best_avg_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
