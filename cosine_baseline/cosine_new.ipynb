{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71869072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_process(req_path, test_path):\n",
    "    req_df = pd.read_csv(req_path, sep=',', on_bad_lines='skip')\n",
    "    test_df = pd.read_csv(test_path, sep=',', on_bad_lines='skip')\n",
    "    \n",
    "    # Some Purpose columns are intentionally left blank for now; populate them with empty strings\n",
    "    test_df['Purpose'] = test_df['Purpose'].fillna('')\n",
    "\n",
    "    req_text_fields = ['Feature', 'Description']\n",
    "    test_text_fields = ['Purpose', 'Test steps']\n",
    "\n",
    "    # Combine text columns for similarity matching\n",
    "    req_df['full_text'] = req_df[req_text_fields].astype(str).agg(' '.join, axis=1)\n",
    "    test_df['full_text'] = test_df[test_text_fields].astype(str).agg(' '.join, axis=1)\n",
    "    \n",
    "    # Convert to lists\n",
    "    req_texts = req_df['full_text'].tolist()\n",
    "    test_texts = test_df['full_text'].tolist()\n",
    "\n",
    "    return req_df, test_df, req_texts, test_texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36b58439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(pred_dict, ground_truth_path, test_path, debug=False):\n",
    "    \n",
    "    gt_df = pd.read_csv(ground_truth_path).dropna(subset=['Req ID'])\n",
    "    gt_dict = {\n",
    "        row['Req ID']: list(map(str.strip, str(row['Test ID']).split(','))) if pd.notna(row['Test ID']) else []\n",
    "        for _, row in gt_df.iterrows()\n",
    "    }\n",
    "\n",
    "    ### CONFUSION MATRIX CALCULATION ###\n",
    "    \n",
    "    # Values for confusion matrix\n",
    "    n: int = 0\n",
    "    tp: int = 0\n",
    "    tn: int = 0\n",
    "    fp: int = 0\n",
    "    fn: int = 0\n",
    "\n",
    "    test_test = pd.read_csv(test_path, sep=',', on_bad_lines='skip')\n",
    "    test_text_fields = ['ID', 'Purpose', 'Test steps']\n",
    "    test_test['full_text'] = test_test[test_text_fields].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "    curr_tests: set[str]\n",
    "    curr_tests: set[str] = set(test_test[\"ID\"].astype(str))\n",
    "\n",
    "    frequency_table: dict[bool, dict[str, dict[str, int]]] = {True: {}, False: {}}\n",
    "\n",
    "    for req in set(pred_dict.keys()) | set(gt_dict.keys()):\n",
    "        actual_tests: set[str] = set(pred_dict.get(req, []))\n",
    "        expected_tests: set[str] = set(gt_dict.get(req, []))\n",
    "\n",
    "        # Skip if req ID returned None\n",
    "        if expected_tests is None:\n",
    "            print(f\"Error - {current_dir}: Faulty requirement ID ({req})\")\n",
    "            continue\n",
    "\n",
    "        # Positives\n",
    "        curr_tp_set: set[str] = actual_tests & expected_tests\n",
    "        curr_tp_count: int = len(curr_tp_set)\n",
    "        if debug:\n",
    "            print(f\"Info - \\t\\t({curr_tp_count}) {curr_tp_set = }\")\n",
    "\n",
    "        curr_fp_set: set[str] = actual_tests - expected_tests\n",
    "        curr_fp_count: int = len(curr_fp_set)\n",
    "        if debug:\n",
    "            print(f\"Info - \\t\\t({curr_fp_count}) {curr_fp_set = }\")\n",
    "        \n",
    "        # Negatives\n",
    "        expected_ns: set[str] = curr_tests - expected_tests\n",
    "        actual_ns: set[str] = curr_tests - actual_tests\n",
    "\n",
    "        curr_tn_set: set[str] = actual_ns & expected_ns\n",
    "        curr_tn_count: int = len(curr_tn_set)\n",
    "        if debug:\n",
    "            print(f\"Info - \\t\\t({curr_tn_count}) {curr_tn_set = }\")\n",
    "\n",
    "        curr_fn_set: set[str] = actual_ns - expected_ns\n",
    "        curr_fn_count: int = len(curr_fn_set)\n",
    "        if debug:\n",
    "            print(f\"Info - \\t\\t({curr_fn_count}) {curr_fn_set = }\")\n",
    "\n",
    "        curr_n: int = curr_tp_count + curr_fp_count + curr_tn_count + curr_fn_count\n",
    "        \n",
    "        # Check so only the right amount of trace links were detected\n",
    "        expected_curr_n: int = len(curr_tests)\n",
    "        if curr_n != expected_curr_n:\n",
    "            print(f\"Error - \\t\\tExpected curr_n = {expected_curr_n}, got {curr_n = }\")\n",
    "        if debug:\n",
    "            print(f\"Info - \\t\\t{curr_n = }\")\n",
    "\n",
    "        # Update the frequency table\n",
    "\n",
    "        # Get the true positives\n",
    "        true_positives: dict[str, int] = frequency_table[True].get(req, None)\n",
    "        # Assign a dict if one doesn't exist\n",
    "        if true_positives is None:\n",
    "            true_positives = {}\n",
    "            frequency_table[True][req] = true_positives\n",
    "        \n",
    "        # Get the false positives\n",
    "        false_positives: dict[str, int] = frequency_table[False].get(req, None)\n",
    "        # Assign a dict if one doesn't exist\n",
    "        if false_positives is None:\n",
    "            false_positives = {}\n",
    "            frequency_table[False][req] = false_positives\n",
    "\n",
    "        # Add 1 for each true positive link\n",
    "        for test in curr_tp_set:\n",
    "            true_positives[test] = true_positives.get(test, 0) + 1\n",
    "\n",
    "        # Add 1 for each false positive link\n",
    "        for test in curr_fp_set:\n",
    "            false_positives[test] = false_positives.get(test, 0) + 1\n",
    "\n",
    "\n",
    "        n += curr_n\n",
    "        tp += curr_tp_count\n",
    "        tn += curr_tn_count\n",
    "        fp += curr_fp_count\n",
    "        fn += curr_fn_count\n",
    "    \n",
    "    accuracy: float = (tp + tn) / n if n != 0 else 0.0\n",
    "    recall: float = tp / (tp + fn) if tp + fn != 0 else 0.0\n",
    "    precision: float = tp / (tp + fp) if tp + fp != 0 else 0.0\n",
    "    specificity: float = tn / (tn + fn) if tn + fn != 0 else 0.0\n",
    "    balanced_accuracy: float = (precision + specificity) / 2\n",
    "    f1: float = 2 * (recall * precision) / (recall + precision) if recall + precision != 0 else 0.0\n",
    "\n",
    "    if debug:\n",
    "        print(\"*********************************************\")\n",
    "        print(ground_truth_path)\n",
    "\n",
    "    if debug:\n",
    "        print(\"*********************************************\")\n",
    "        print(f\"Accuracy: {balanced_accuracy:.2f}%\")\n",
    "        print(\"*********************************************\")\n",
    "\n",
    "  \n",
    "    # assert tp + fn == 134\n",
    "\n",
    "    return tp, fp, tn, fn, accuracy, recall, precision, balanced_accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23dc9cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def custom_preprocessor(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "\n",
    "def compute_cosine(req_df, test_df, req_texts, test_texts, threshold):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        preprocessor=custom_preprocessor,\n",
    "        tokenizer=custom_tokenizer,\n",
    "        token_pattern=None\n",
    "    )\n",
    "\n",
    "    documents = req_texts + test_texts\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix[:len(req_texts)], tfidf_matrix[len(req_texts):])\n",
    "\n",
    "    matches = {}\n",
    "    for i, req_id in enumerate(req_df['ID']):\n",
    "        matched_test_ids = []\n",
    "        for j, similarity in enumerate(similarity_matrix[i]):\n",
    "            if similarity >= threshold:\n",
    "                matched_test_ids.append(str(test_df.iloc[j]['ID']))\n",
    "        matches[str(req_id)] = matched_test_ids\n",
    "        \n",
    "    print(f\"Matches for threshold {threshold:.2f}: {len(matches)}\")\n",
    "    print(f\"Matches: {matches}\")\n",
    "\n",
    "    return matches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e13f185",
   "metadata": {},
   "source": [
    "### Logic including subsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bbe4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: AMINA. Subset: 01\n",
      "Threshold: 0.34\n",
      "Matches for threshold 0.34: 25\n",
      "Matches: {'S1': ['406'], 'S2': ['35'], 'S12': ['364'], 'S24': ['43', '44', '248', '249'], 'S56': ['245'], 'S59': ['248', '249', '250'], 'S75': ['451', '452'], 'S89': [], 'S116': [], 'S135': ['216'], 'S177': ['352'], 'S182': ['480', '486'], 'S234': ['21'], 'S236': ['23'], 'B53': ['434'], 'B85': ['238'], 'B87': ['271'], 'B93': ['281'], 'B105': ['10'], 'B106': ['642'], 'B108': ['32'], 'S549': ['90'], 'S555': ['96'], 'S557': ['98'], 'S558': ['99']}\n",
      "*********************************************\n",
      "Dataset: AMINA. Subset: 02\n",
      "Threshold: 0.34\n",
      "Matches for threshold 0.34: 25\n",
      "Matches: {'S2': ['35'], 'S12': ['364'], 'S27': ['414', '415'], 'S28': ['63', '485'], 'S30': ['483', '484'], 'S38': ['71', '331', '596'], 'S75': ['451', '452'], 'S111': ['188'], 'S117': [], 'S135': ['216'], 'S137': ['220'], 'S143': ['596'], 'S148': ['216', '256'], 'S152': ['260'], 'S163': ['273'], 'S174': ['348'], 'S236': ['23'], 'B24': ['319'], 'B34': ['331'], 'B45': ['465', '466'], 'B61': ['358'], 'B90': ['274'], 'B108': ['32'], 'S537': ['491'], 'S549': ['90']}\n",
      "*********************************************\n",
      "Dataset: AMINA. Subset: 03\n",
      "Threshold: 0.34\n",
      "Matches for threshold 0.34: 25\n",
      "Matches: {'S1': ['406'], 'S11': ['362'], 'S60': ['251'], 'S72': ['445', '446'], 'S86': ['430', '478'], 'S89': [], 'S137': ['220'], 'S148': ['256'], 'S166': ['277'], 'S172': [], 'S231': ['20'], 'S234': ['21'], 'S236': ['23'], 'S241': ['646'], 'B48': ['471', '472'], 'B71': ['218'], 'B90': ['274'], 'B93': ['281'], 'B106': ['642'], 'B108': ['32'], 'B113': [], 'S543': ['84'], 'S545': ['86'], 'S548': ['89'], 'S558': ['99']}\n",
      "*********************************************\n",
      "Dataset: AMINA. Subset: 04\n",
      "Threshold: 0.34\n",
      "Matches for threshold 0.34: 25\n",
      "Matches: {'S1': ['406'], 'S28': ['63', '485'], 'S56': ['245'], 'S75': ['451', '452'], 'S77': ['455', '456'], 'S116': [], 'S139': [], 'S147': ['255'], 'S148': ['256'], 'S152': ['260'], 'S155': ['263'], 'S172': [], 'S221': ['653'], 'S228': ['33'], 'S231': ['20'], 'B11': ['302', '303', '304', '330'], 'B33': ['302', '303', '304', '330'], 'B48': ['471', '472'], 'B71': ['218'], 'B87': ['271'], 'B91': ['279'], 'B106': ['642'], 'S546': ['87'], 'S548': ['89'], 'S558': ['99']}\n",
      "*********************************************\n",
      "Dataset: AMINA. Subset: 05\n",
      "Threshold: 0.34\n",
      "Matches for threshold 0.34: 25\n",
      "Matches: {'S12': ['364'], 'S24': ['43', '44'], 'S70': ['441', '442'], 'S73': ['447', '448'], 'S117': [], 'S136': ['219', '615'], 'S143': ['596'], 'S155': ['263'], 'S172': [], 'S177': ['352'], 'S231': ['20'], 'S237': [], 'S259': [], 'B31': ['328'], 'B33': ['330'], 'B42': ['339', '340', '341'], 'B62': ['194', '364'], 'B71': ['218'], 'B85': ['238'], 'B105': ['10'], 'B113': [], 'S545': ['86'], 'S549': ['90'], 'S558': ['99'], 'S560': ['101']}\n",
      "*********************************************\n",
      "Dataset: AMINA. Subset: 06\n",
      "Threshold: 0.34\n",
      "Matches for threshold 0.34: 25\n",
      "Matches: {'S1': ['406'], 'S58': ['247'], 'S59': ['248', '249', '250'], 'S72': ['445', '446'], 'S75': ['451', '452'], 'S88': ['432'], 'S91': ['143'], 'S116': [], 'S119': [], 'S139': ['224'], 'S148': ['256'], 'S171': [], 'S215': ['58'], 'S221': ['653'], 'S228': ['33'], 'S231': ['20'], 'B30': ['327'], 'B34': ['331'], 'B73': ['224'], 'B85': ['238', '256'], 'B105': ['10'], 'B108': ['32'], 'S537': ['491'], 'S543': ['84'], 'S549': ['90']}\n",
      "*********************************************\n",
      "Dataset: AMINA. Subset: 07\n",
      "Threshold: 0.34\n",
      "Matches for threshold 0.34: 25\n",
      "Matches: {'S8': ['360'], 'S60': ['251'], 'S73': ['447', '448'], 'S89': ['625'], 'S94': ['149'], 'S135': ['216'], 'S137': ['220'], 'S143': ['596'], 'S152': ['260'], 'S163': ['273'], 'S182': ['480', '486'], 'S228': ['33'], 'S234': ['21'], 'S238': ['24'], 'B1': ['293'], 'B24': ['319', '328'], 'B30': ['327', '328'], 'B31': ['319', '327', '328'], 'B53': ['434'], 'B106': ['642'], 'B108': ['32'], 'S537': ['491'], 'S543': ['84'], 'S558': ['99'], 'S559': ['100']}\n",
      "*********************************************\n",
      "Dataset: AMINA. Subset: 08\n",
      "Threshold: 0.34\n",
      "Matches for threshold 0.34: 25\n",
      "Matches: {'S28': ['63', '485'], 'S86': ['430', '478'], 'S89': [], 'S94': ['149'], 'S147': ['255'], 'S163': ['273'], 'S172': [], 'S174': ['348'], 'S186': ['489'], 'S192': ['479'], 'S236': ['23'], 'S259': [], 'B15': ['310'], 'B24': ['319', '330'], 'B33': ['319', '330'], 'B48': ['471', '472'], 'B61': ['358'], 'B81': ['237'], 'B87': ['271'], 'B90': ['274'], 'B106': ['642'], 'B108': ['32'], 'S545': ['86'], 'S557': ['98'], 'S558': ['99']}\n",
      "*********************************************\n",
      "Dataset: AMINA. Subset: 09\n",
      "Threshold: 0.34\n",
      "Matches for threshold 0.34: 25\n",
      "Matches: {'S2': ['35'], 'S12': ['255', '364'], 'S24': ['43', '44'], 'S73': ['447', '448'], 'S77': ['455', '456'], 'S111': ['188'], 'S147': ['255', '364'], 'S152': ['260'], 'S166': ['277'], 'S172': [], 'S213': ['56'], 'S228': ['33'], 'S236': ['23'], 'S237': [], 'B1': ['293'], 'B15': ['310'], 'B24': ['319'], 'B34': ['331'], 'B42': ['339', '340', '341'], 'B61': ['358'], 'B71': ['218'], 'B85': ['238'], 'B90': ['274'], 'B105': ['10'], 'S546': ['87']}\n",
      "*********************************************\n",
      "Dataset: AMINA. Subset: 10\n",
      "Threshold: 0.34\n",
      "Matches for threshold 0.34: 25\n",
      "Matches: {'S28': ['63', '485'], 'S30': ['483', '484'], 'S58': ['247'], 'S60': ['251'], 'S73': ['447', '448'], 'S81': ['463', '464'], 'S94': ['149'], 'S116': [], 'S117': [], 'S119': [], 'S135': ['216'], 'S139': ['224'], 'S172': [], 'S174': ['348'], 'S192': ['479'], 'S236': ['23'], 'B15': ['310'], 'B34': ['331'], 'B35': ['332'], 'B46': ['467', '468'], 'B73': ['224'], 'B85': ['238'], 'B87': ['271'], 'B91': ['279'], 'S560': ['101']}\n",
      "*********************************************\n",
      "\n",
      "\n",
      "\n",
      "Best Metrics per Subset (Based on F1 Score):\n",
      "+------------------+----------+----------------+----------+---------+----------+---------+------------+--------+----------+-------------------+-------------------+\n",
      "| Dataset          | Subset   | Threshold      | TP       |FP       | TN       |FN       |Accuracy    | Recall | Precision| Balanced Accuracy |      F1 score     |\n",
      "+------------------+----------+----------------+----------+---------+----------+---------+------------+--------+----------+-------------------+-------------------+\n",
      "| AMINA            | 01       | 0.34           | 28       | 2       | 957      | 13      | 0.98       | 0.68   | 0.93     | 0.959966          | 0.788732          |\n",
      "| AMINA            | 02       | 0.34           | 29       | 3       | 741      | 2       | 0.99       | 0.94   | 0.91     | 0.951779          | 0.920635          |\n",
      "| AMINA            | 03       | 0.34           | 25       | 0       | 911      | 14      | 0.99       | 0.64   | 1.00     | 0.992432          | 0.781250          |\n",
      "| AMINA            | 04       | 0.34           | 28       | 4       | 740      | 3       | 0.99       | 0.90   | 0.88     | 0.935481          | 0.888889          |\n",
      "| AMINA            | 05       | 0.34           | 26       | 1       | 743      | 5       | 0.99       | 0.84   | 0.96     | 0.978139          | 0.896552          |\n",
      "| AMINA            | 06       | 0.34           | 25       | 2       | 718      | 5       | 0.99       | 0.83   | 0.93     | 0.959505          | 0.877193          |\n",
      "| AMINA            | 07       | 0.34           | 27       | 4       | 884      | 10      | 0.98       | 0.73   | 0.87     | 0.929891          | 0.794118          |\n",
      "| AMINA            | 08       | 0.34           | 25       | 2       | 934      | 14      | 0.98       | 0.64   | 0.93     | 0.955579          | 0.757576          |\n",
      "| AMINA            | 09       | 0.34           | 28       | 2       | 717      | 3       | 0.99       | 0.90   | 0.93     | 0.964583          | 0.918033          |\n",
      "| AMINA            | 10       | 0.34           | 25       | 1       | 767      | 7       | 0.99       | 0.78   | 0.96     | 0.976247          | 0.862069          |\n",
      "+----------------------+------------------+----------------+----------+---------+----------+---------+------------+--------+----------+-------------------+-------------------+\n",
      " Threshold  F1 score   Recall  Precision\n",
      "      0.34  0.848505 0.788994   0.929524\n",
      "üèÜ Best global threshold: 0.34 with average F1 score: 0.8485\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Datasets folders \n",
    "datasets = [\"AMINA\"]\n",
    "subsets = [f\"{i:02}\" for i in range(1, 11)]  # 01 to 10\n",
    "summary_table = pd.DataFrame()\n",
    "thresholds = [0.34]\n",
    "# thresholds = np.arange(0.01, 1.01, 0.01)\n",
    "\n",
    "# For every dataset find the following:\n",
    "for dataset in datasets:    \n",
    "    for subset in subsets:\n",
    "        req_path = f'./data/{dataset}/{subset}/RE.csv'\n",
    "        test_path = f'./data/{dataset}/{subset}/ST.csv'\n",
    "        ground_truth_mapping_path = f'../data/{dataset}/{subset}/mapping.csv'\n",
    "        \n",
    "        print(f\"Dataset: {dataset}. Subset: {subset}\")\n",
    "        \n",
    "        # Load and process data\n",
    "        req_df, test_df, req_texts, test_texts = load_and_process(req_path, test_path)\n",
    "        \n",
    "        # Find cosine similarity and measure execution time\n",
    "        start = time.time()\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            threshold_str = f\"{threshold:.2f}\"\n",
    "            print(f\"Threshold: {threshold:.2f}\")\n",
    "            predicted_matches = compute_cosine(req_df, test_df, req_texts, test_texts, threshold)\n",
    "            t = time.time() - start\n",
    "            tp, fp, tn, fn, accuracy, recall, precision, balanced_accuracy, f1 = calculate_accuracy(predicted_matches, ground_truth_mapping_path, test_path, debug=False)\n",
    "            summary_table = pd.concat([\n",
    "                summary_table,\n",
    "                pd.DataFrame([{\"Dataset\": dataset, \"Subset\": subset, \"Threshold\": f\"{threshold:.2f}\", \"TP\": tp, \"FP\": fp, \"TN\": tn, \"FN\": fn, \"Accuracy\": accuracy, \"Recall\": recall, \"Precision\": precision, \"Balanced Accuracy\": balanced_accuracy, \"F1 score\": f1}])\n",
    "            ], ignore_index=True)\n",
    "            print(\"*********************************************\")\n",
    "\n",
    "        \n",
    "            payload: dict[str, dict] = {\n",
    "                \"meta\": {\n",
    "                    \"req_path\": req_path,\n",
    "                    \"test_path\": test_path,\n",
    "                    \"mapping_path\": ground_truth_mapping_path\n",
    "                },\n",
    "                \"data\": {\n",
    "                    \"links\": predicted_matches,\n",
    "                    \"time_to_analyze\": t,\n",
    "                    \"err\": []\n",
    "                }\n",
    "            }\n",
    "            current_time = time.strftime(\"%H_%M_%S\", time.localtime())\n",
    "            current_date = time.strftime(\"%Y-%m-%d\", time.localtime())\n",
    "\n",
    "            log_dir: str = f\"./MIS_COSINE_{dataset}/{current_date}/{current_time}/{subset}\"\n",
    "            os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "            with open(f\"{log_dir}/res.json\", \"w+\") as out:\n",
    "                json.dump(payload, out, indent=2)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# Convert Threshold back to float for sorting if necessary\n",
    "summary_table[\"Threshold\"] = summary_table[\"Threshold\"].astype(float)\n",
    "summary_table = summary_table.sort_values(by=[\"Threshold\", \"Dataset\", \"Subset\"])\n",
    "grouped = summary_table.groupby([\"Threshold\"], as_index=False).agg({\"F1 score\": \"mean\", \"Recall\": \"mean\", \"Precision\": \"mean\"})\n",
    "grouped = grouped.sort_values(by=[\"F1 score\", \"Recall\", \"Precision\"], ascending=[False, False, False])\n",
    "\n",
    "# avg_f1_per_threshold = grouped.groupby(\"Threshold\", as_index=False).agg({\"F1 score\": \"mean\"})\n",
    "best_row = grouped.loc[grouped[\"F1 score\"].idxmax()]\n",
    "\n",
    "# Print Best Metrics per Subset (based on max F1 score)\n",
    "print(\"\\nBest Metrics per Subset (Based on F1 Score):\")\n",
    "print(\"+------------------+----------+----------------+----------+---------+----------+---------+------------+--------+----------+-------------------+-------------------+\")\n",
    "print(\"| Dataset          | Subset   | Threshold      | TP       |FP       | TN       |FN       |Accuracy    | Recall | Precision| Balanced Accuracy |      F1 score     |\")\n",
    "print(\"+------------------+----------+----------------+----------+---------+----------+---------+------------+--------+----------+-------------------+-------------------+\")\n",
    "for _, row in summary_table.iterrows():\n",
    "    print(\n",
    "        f\"| {row['Dataset']:<16} \"\n",
    "        f\"| {row['Subset']:<8} \"\n",
    "        f\"| {row['Threshold']:<14} \"\n",
    "        f\"| {int(row['TP']):<8} \"\n",
    "        f\"| {int(row['FP']):<7} \"\n",
    "        f\"| {int(row['TN']):<8} \"\n",
    "        f\"| {int(row['FN']):<7} \"\n",
    "        f\"| {row['Accuracy']:<10.2f} \"\n",
    "        f\"| {row['Recall']:<6.2f} \"\n",
    "        f\"| {row['Precision']:<8.2f} \"\n",
    "        f\"| {row['Balanced Accuracy']:<18f}\"\n",
    "        f\"| {row['F1 score']:<18f}|\")\n",
    "print(\"+----------------------+------------------+----------------+----------+---------+----------+---------+------------+--------+----------+-------------------+-------------------+\")\n",
    "\n",
    "print(grouped.to_string(index=False))\n",
    "\n",
    "best_threshold = best_row[\"Threshold\"]\n",
    "best_avg_f1 = best_row[\"F1 score\"]\n",
    "print(f\"üèÜ Best global threshold: {best_threshold:.2f} with average F1 score: {best_avg_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
