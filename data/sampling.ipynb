{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b53bd7",
   "metadata": {},
   "source": [
    "## `TL;DR`: How sampling works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef005f6f",
   "metadata": {},
   "source": [
    "1. Iterate over each dataset directory; the directory name corresponds with the `<dataset_name>`\n",
    "\n",
    "2. Load the dataset CSV-files (`RE.csv`, `ST.csv`, `mapping.csv`) and create dataframes for each\n",
    "\n",
    "3. Randomly generate `number_of_subsets` subsets by selecting one of two sampling functions; controlled using the `use_primitive` boolean variable:\n",
    "> **Hint**: the key difference lies in how the functions deal with `one-to-many` / `many-to-many` mappings. To include additional indirectly connected requirements and tests select the first alternative; note that this may increase the sample size.\n",
    "   - Option 1: `sample_data()` - recursive; subset size >= sample_size argument:\n",
    "      1. Randomly sample *`n`* requirements (ID's)\n",
    "      2. Use the mappings to find all related tests (ID's)\n",
    "      3. Check if any tests are related to more than one requirement \n",
    "      4. Recursively add any indirectly connected requirements and check their connected tests as well\n",
    "      5. Repeat until all directly or indirectly connected requirements and tests have been identified\n",
    "      6. Filter and return the new subset (`requirements`, `tests`, and `mappings` dataframes)\n",
    "\n",
    "  - Option 2: `sample_data_primitive()` - non-recursive alternative; strictly respect sample_size argument:\n",
    "    1. Randomly sample *`n`* requirements (ID's)\n",
    "    2. Use the mappings to find all related tests (ID's)\n",
    "    3. Filter and return the new subset (`requirements`, `tests`, and `mappings` dataframes)\n",
    "\n",
    "4. Store the subsets within the corresponding `dataset` directory:\n",
    "    - subset directories use an index naming convention starting at `01`\n",
    "    - each subset directory contains three files: `RE.csv`, `ST.csv`, and `mapping.csv`\n",
    "\n",
    "**Potential TODO's**:\n",
    "- Add additional code to find how many different mappings are in each dataset / subset? Save these mappings to CSV?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e3a2bf",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32a128cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68196a7b",
   "metadata": {},
   "source": [
    "## Read & store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddfc3892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_requirements(directory: str) -> pd.DataFrame:\n",
    "    file_path = os.path.join(directory, \"RE.csv\")\n",
    "    return pd.read_csv(file_path, dtype=str, on_bad_lines=\"warn\")\n",
    "\n",
    "def read_tests(directory: str) -> pd.DataFrame:\n",
    "    file_path = os.path.join(directory, \"ST.csv\")\n",
    "    test_df = pd.read_csv(file_path, dtype=str, on_bad_lines=\"warn\")\n",
    "    \n",
    "    # Some Purpose columns are intentionally left blank; populate them with empty strings\n",
    "    test_df[\"Purpose\"] = test_df[\"Purpose\"].fillna(\"\")\n",
    "    return test_df\n",
    "\n",
    "def read_mappings(directory: str) -> pd.DataFrame:\n",
    "    file_path = os.path.join(directory, \"mapping.csv\")\n",
    "    return pd.read_csv(file_path, dtype=str, on_bad_lines=\"warn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5a3aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_subset_data(requirements: pd.DataFrame, tests: pd.DataFrame, mappings: pd.DataFrame, output_dir: str, subset_index: int):\n",
    "    # Output directory uses an integer index denoting the subset\n",
    "    prefix = \"0\" if subset_index < 10 else \"\" # Prefix with 0 if single digit\n",
    "    output_dir = f\"{output_dir}/{prefix}{subset_index}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    requirements.to_csv(os.path.join(output_dir, f\"RE.csv\"), index=False)\n",
    "    tests.to_csv(os.path.join(output_dir, f\"ST.csv\"), index=False)\n",
    "    mappings.to_csv(os.path.join(output_dir, f\"mapping.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608a3571",
   "metadata": {},
   "source": [
    "### Sampling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d5f3fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_include_connected(req_id: str, requirements_set: set, tests_set: set, mappings: dict[str, set]):\n",
    "    \"\"\"\n",
    "    Helper function for `sample_data()`.\n",
    "\n",
    "    Recursively identifies all directly or indirectly connected system test cases and requirements -  \n",
    "    adds each identified artifact to the `requirements_set` or `tests_set` respectively.\n",
    "    \"\"\"\n",
    "    # Base case: current requirement has already been processed\n",
    "    if req_id in requirements_set: \n",
    "        return\n",
    "    \n",
    "    # Add the current req. ID to the set of requirement ID's\n",
    "    requirements_set.add(req_id)\n",
    "\n",
    "    # Find the list of tests that map to the requirement\n",
    "    connected_tests = mappings[req_id]\n",
    "\n",
    "    # Add connected tests to the set of tests\n",
    "    tests_set |= connected_tests\n",
    "\n",
    "    # Recursively check if any connected test maps to more than one requirement\n",
    "    for test_id in connected_tests:\n",
    "        # Requirement ID's connected to the current test\n",
    "        connected_reqs = [req for req in mappings if test_id in mappings[req]]\n",
    "\n",
    "        for req in connected_reqs:\n",
    "            recursive_include_connected(req, requirements_set, tests_set, mappings)\n",
    "\n",
    "\n",
    "def sample_data(requirements: pd.DataFrame, tests: pd.DataFrame, mappings: pd.DataFrame, sample_size: int) -> \\\n",
    "        tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Randomly samples a subset of requirement specifications and recursively includes all \n",
    "    directly or indirectly connected system test cases and requirements \n",
    "    (e.g., through through one-to-many or many-to-one relationships).\n",
    "\n",
    "    This means the final subset may include more requirements than specified by `sample_size`, \n",
    "    depending on the interconnections in the mapping data. \n",
    "    \"\"\"\n",
    "    # Convert ground truth data into a dictionary where keys are req_id and values are sets of test_ids\n",
    "    mappings_dict: dict[str, set] = {\n",
    "        row[\"Req ID\"]: # key\n",
    "        set(map(str.strip, str(row[\"Test ID\"]).split(\",\"))) if pd.notna(row[\"Test ID\"]) else set() # value \n",
    "        for _, row in mappings.iterrows()\n",
    "    }\n",
    "    \n",
    "    # Sample \"n\" random requirement ID's\n",
    "    sampled_requirements: set[str] = set(random.sample(list(mappings[\"Req ID\"]), sample_size))\n",
    "\n",
    "    # Sets to hold the sampled requirement and test ID's\n",
    "    sampled_requirements_set: set[str] = set()\n",
    "    sampled_tests_set: set[str] = set()\n",
    "\n",
    "    # Recursively find any connected tests and requirements; add them to the corresponding set\n",
    "    for req_id in sampled_requirements:\n",
    "        recursive_include_connected(req_id, sampled_requirements_set, sampled_tests_set, mappings_dict)\n",
    "\n",
    "    # Create filtered dataframes containing only the sampled test, requirements and mappings\n",
    "    filtered_reqs = requirements[requirements[\"ID\"].isin(sampled_requirements_set)]\n",
    "    filtered_tests = tests[tests[\"ID\"].isin(sampled_tests_set)]\n",
    "    filtered_mapping = mappings[\n",
    "            # Find all rows that contain a sampled req_id or test_id\n",
    "            mappings[\"Req ID\"].isin(sampled_requirements_set) | \n",
    "            mappings[\"Test ID\"].apply(\n",
    "                    lambda test_id_str: bool(sampled_tests_set & set(map(str.strip, test_id_str.split(\",\"))))\n",
    "                    if pd.notna(test_id_str) else False\n",
    "                )\n",
    "        ]\n",
    "    \n",
    "    return filtered_reqs, filtered_tests, filtered_mapping\n",
    "\n",
    "\n",
    "def sample_data_primitive(requirements: pd.DataFrame, tests: pd.DataFrame, mappings: pd.DataFrame, sample_size: int) -> \\\n",
    "        tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\" \n",
    "    Randomly samples a subset of requirement specifications and includes only the directly \n",
    "    connected system test cases based on the provided mappings.\n",
    "\n",
    "    Unlike the recursive version, this function does not follow indirect connections, \n",
    "    so the returned subset strictly reflects the initially sampled requirements.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert ground truth data into a dictionary where keys are req_id and values are sets of test_ids\n",
    "    mappings_dict: dict[str, set] = {\n",
    "        row[\"Req ID\"]: # key\n",
    "        set(map(str.strip, str(row[\"Test ID\"]).split(\",\"))) if pd.notna(row[\"Test ID\"]) else set() # value \n",
    "        for _, row in mappings.iterrows()\n",
    "    }\n",
    "\n",
    "    # Sample \"n\" random requirement ID's\n",
    "    sampled_requirements: set[str] = set(random.sample(list(mappings[\"Req ID\"]), sample_size))\n",
    "    \n",
    "    # Set to hold the sampled test ID's\n",
    "    sampled_tests: set[str] = set()\n",
    "\n",
    "    # Find all test ID's connected to the sampled requirements\n",
    "    for req_id in sampled_requirements:\n",
    "        sampled_tests |= mappings_dict[req_id]\n",
    "\n",
    "    # Create filtered dataframes containing only the sampled test, requirements and mappings\n",
    "    filtered_reqs = requirements[requirements[\"ID\"].isin(sampled_requirements)]\n",
    "    filtered_tests = tests[tests[\"ID\"].isin(sampled_tests)]\n",
    "    filtered_mapping = mappings[\n",
    "            # Find all rows that contain a sampled req_id\n",
    "            mappings[\"Req ID\"].isin(sampled_requirements)\n",
    "        ]\n",
    "\n",
    "    return filtered_reqs, filtered_tests, filtered_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ca7ea5",
   "metadata": {},
   "source": [
    "## Main execution cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e319e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_dir = \".\" # Datasets are currently stored in the same directory as this Jupyter Notebook\n",
    "\n",
    "number_of_subsets = 10\n",
    "# TODO: Placeholder value\n",
    "sample_size = 3 \n",
    "# The datasets have vastly different population sizes which makes it hard to use the same sample size\n",
    "# Either work around by hard-coding some values (array?) corresponding with the datasets, or\n",
    "# make some dynamic logic to create a sample based on a proportion of the population i.e. some percentage\n",
    "\n",
    "use_primitive = False # Determines sampling function\n",
    "\n",
    "sampling_function = sample_data_primitive if use_primitive else sample_data # Select function pointer\n",
    "\n",
    "for dataset in os.listdir(datasets_dir):\n",
    "    # Construct filepath to the current dataset\n",
    "    dataset_path = os.path.join(datasets_dir, dataset)\n",
    "\n",
    "    # Skip current iteration if we encountered a file\n",
    "    if not os.path.isdir(dataset_path): continue\n",
    "    \n",
    "    requirements = read_requirements(dataset_path)\n",
    "    tests        = read_tests(dataset_path)\n",
    "    mappings     = read_mappings(dataset_path)\n",
    "    \n",
    "    # Shift the entire range by 1 so that indexing starts at 1\n",
    "    for i in range(1, number_of_subsets + 1):\n",
    "        # Sample dataset to create new subset\n",
    "        sampled_reqs, sampled_tests, sampled_mappings = sampling_function(\n",
    "            requirements, tests, mappings, sample_size\n",
    "        )\n",
    "\n",
    "        # Save the sampled data to CSV-file, use i as subset index\n",
    "        save_subset_data(sampled_reqs, sampled_tests, sampled_mappings, dataset_path, i)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540297d7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5add5617",
   "metadata": {},
   "source": [
    "### [Extra] execution cell to manually sample 100 Mozilla requirements\n",
    "\n",
    "Uses the *primitive* sampling method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb8a365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ITERATION: 1\n",
      "Number of rows in requirements DataFrame: 25\n",
      "Number of rows in tests DataFrame: 25\n",
      "Number of rows in mappings DataFrame: 25\n",
      "\n",
      "ITERATION: 2\n",
      "Number of rows in requirements DataFrame: 25\n",
      "Number of rows in tests DataFrame: 21\n",
      "Number of rows in mappings DataFrame: 25\n",
      "\n",
      "ITERATION: 3\n",
      "Number of rows in requirements DataFrame: 25\n",
      "Number of rows in tests DataFrame: 20\n",
      "Number of rows in mappings DataFrame: 25\n",
      "\n",
      "ITERATION: 4\n",
      "Number of rows in requirements DataFrame: 25\n",
      "Number of rows in tests DataFrame: 20\n",
      "Number of rows in mappings DataFrame: 25\n",
      "\n",
      "ITERATION: 5\n",
      "Number of rows in requirements DataFrame: 25\n",
      "Number of rows in tests DataFrame: 23\n",
      "Number of rows in mappings DataFrame: 25\n",
      "\n",
      "ITERATION: 6\n",
      "Number of rows in requirements DataFrame: 25\n",
      "Number of rows in tests DataFrame: 23\n",
      "Number of rows in mappings DataFrame: 25\n",
      "\n",
      "ITERATION: 7\n",
      "Number of rows in requirements DataFrame: 25\n",
      "Number of rows in tests DataFrame: 22\n",
      "Number of rows in mappings DataFrame: 25\n",
      "\n",
      "ITERATION: 8\n",
      "Number of rows in requirements DataFrame: 25\n",
      "Number of rows in tests DataFrame: 21\n",
      "Number of rows in mappings DataFrame: 25\n",
      "\n",
      "ITERATION: 9\n",
      "Number of rows in requirements DataFrame: 25\n",
      "Number of rows in tests DataFrame: 18\n",
      "Number of rows in mappings DataFrame: 25\n",
      "\n",
      "ITERATION: 10\n",
      "Number of rows in requirements DataFrame: 25\n",
      "Number of rows in tests DataFrame: 19\n",
      "Number of rows in mappings DataFrame: 25\n"
     ]
    }
   ],
   "source": [
    "number_of_subsets = 10\n",
    "sample_size = 25 \n",
    "\n",
    "dataset_path = \"./deprecated_datasets/AMINA-100/\"\n",
    "output_path = \"./AMINA/\"\n",
    "\n",
    "requirements = read_requirements(dataset_path)\n",
    "tests        = read_tests(dataset_path)\n",
    "mappings     = read_mappings(dataset_path)\n",
    "\n",
    "# Shift the entire range by 1 so that indexing starts at 1\n",
    "for i in range(1, number_of_subsets + 1):\n",
    "    # Sample dataset to create new subset\n",
    "    sampled_reqs, sampled_tests, sampled_mappings = sample_data_primitive(\n",
    "        requirements, tests, mappings, sample_size\n",
    "    )\n",
    "\n",
    "    print(f\"\\nITERATION: {i}\")\n",
    "    print(\"Number of rows in requirements DataFrame:\", sampled_reqs.shape[0])\n",
    "    print(\"Number of rows in tests DataFrame:\", sampled_tests.shape[0])\n",
    "    print(\"Number of rows in mappings DataFrame:\", sampled_mappings.shape[0])\n",
    "\n",
    "    # Save the sampled data to CSV-file, use i as subset index\n",
    "    save_subset_data(sampled_reqs, sampled_tests, sampled_mappings, output_path, i)     \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
