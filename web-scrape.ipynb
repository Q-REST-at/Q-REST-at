{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e4b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "BASE_URL = \"https://www-archive.mozilla.org\"\n",
    "PAGE_URL_TEMPLATE = \"https://www-archive.mozilla.org/quality/browser/front-end/testcases/\"\n",
    "\n",
    "def scrape_test_page(page_url, req_counter, test_counter, path):\n",
    "\n",
    "    response = requests.get(page_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    requirements = []\n",
    "    tests = []\n",
    "    mappings = []\n",
    "\n",
    "    def extract_section(soup, section_name):\n",
    "        \"\"\"Extracts text content of the section with <h2>section_name</h2>\"\"\"\n",
    "        h2 = soup.find(\"h2\", string=lambda text: text and section_name.lower() in text.lower())\n",
    "        content_text = \"\"\n",
    "        \n",
    "        if h2:\n",
    "            section_title = h2.get_text(strip=True)\n",
    "            next_elem = h2.find_next_sibling()\n",
    "            parts = []\n",
    "            while next_elem and not (next_elem.name == 'h2'):\n",
    "                parts.append(next_elem.get_text(separator=' ', strip=True))\n",
    "                next_elem = next_elem.find_next_sibling()\n",
    "\n",
    "            full_section = [section_title] + parts\n",
    "            content_text = \"\\n\".join(full_section).strip()\n",
    "                    \n",
    "        return content_text\n",
    "\n",
    "    def build_full_link(relative_link):\n",
    "        current_page_url = urljoin(PAGE_URL_TEMPLATE, path)\n",
    "        if not current_page_url.endswith(\"/\"):\n",
    "            current_page_url += \"/\"\n",
    "\n",
    "        # Count slashes to determine if relative_link is a short name\n",
    "        if relative_link:\n",
    "            if relative_link.count('/') <= 1:  \n",
    "                # E.g. relative_link is \"Streamlined%20interface.htm\", then build full path\n",
    "                return urljoin(current_page_url, relative_link)\n",
    "            else:\n",
    "                # E.g. relative_link is /quality/browser/front-end/testcases/help/tssignon\n",
    "                # then just concatenate is to the base url\n",
    "                return urljoin(BASE_URL, relative_link)\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "\n",
    "    for row in soup.find_all(\"tr\"):\n",
    "        tds = row.find_all(\"td\")\n",
    "        parsed_link = \"\"\n",
    "\n",
    "        # For  \"drag-drop\", \"form-manager\" page layouts\n",
    "        if len(tds) == 2:\n",
    "            feature_td = tds[0]\n",
    "            description = tds[1].get_text(strip=True)\n",
    "            link_tag = feature_td.find(\"a\")\n",
    "            feature_parts = []\n",
    "            for content in feature_td.contents:\n",
    "                if isinstance(content, str):\n",
    "                    feature_parts.append(content.strip())\n",
    "                elif content.name == 'a':\n",
    "                    feature_parts.append(content.get_text(strip=True))\n",
    "            feature = \" \".join(feature_parts).strip()\n",
    "        # For  \"oji\", \"password-manager\", \"printing\", \"plugins\", \"xpapps-gui\", \"themes\" page layouts\n",
    "        elif len(tds) == 3: \n",
    "            feature = tds[0].get_text(strip=True)\n",
    "            description = tds[1].get_text(strip=True)\n",
    "            link_tag = tds[2].find(\"a\")\n",
    "        # For all other page layouts\n",
    "        elif len(tds) == 4:\n",
    "            feature = tds[1].get_text(strip=True)\n",
    "            description = tds[2].get_text(strip=True)\n",
    "            link_tag = tds[3].find(\"a\")\n",
    "        elif len(tds) < 2:\n",
    "            # Skip rows that are not 2, 3 or 4 columns\n",
    "            continue\n",
    "        \n",
    "        relative_link = link_tag['href'] if link_tag else None\n",
    "        full_link = build_full_link(relative_link)\n",
    "        # Save Requirement\n",
    "        req_id = f\"R-{req_counter}\"\n",
    "        requirements.append({\n",
    "            \"ID\": req_id,\n",
    "            \"Feature\": feature,\n",
    "            \"Description\": description\n",
    "        })\n",
    "\n",
    "        purpose_text = \"\"\n",
    "        steps_text = \"\" \n",
    "        test_steps_combined= \"\"\n",
    "        \n",
    "        if full_link:   \n",
    "            test_response = requests.get(full_link)\n",
    "            test_soup = BeautifulSoup(test_response.content, \"html.parser\")\n",
    "\n",
    "            purpose_text = extract_section(test_soup, \"Purpose\")\n",
    "            initial_conditions = extract_section(test_soup, \"Initial Conditions\")\n",
    "            steps_text = extract_section(test_soup, \"Steps/Description\")\n",
    "            # Some sections are named \"Description\" instead of \"Steps/Description\"    \n",
    "            desc_text = steps_text if steps_text else extract_section(test_soup, \"Description\")\n",
    "            expected_results = extract_section(test_soup, \"Expected Results\")\n",
    "\n",
    "\n",
    "            test_steps_combined = \"\\n\\n\".join(filter(None, [\n",
    "                initial_conditions,\n",
    "                desc_text,\n",
    "                expected_results\n",
    "            ]))\n",
    "\n",
    "        # Save Test\n",
    "        test_id = f\"T-{req_counter}\"\n",
    "        tests.append({\n",
    "            \"ID\": test_id,\n",
    "            \"Purpose\": purpose_text,\n",
    "            \"TestSteps\": test_steps_combined\n",
    "        })\n",
    "\n",
    "        # Save Mapping\n",
    "        mappings.append({\n",
    "            \"ReqID\": req_id,\n",
    "            \"TestID\": test_id\n",
    "        })\n",
    "\n",
    "        req_counter += 1\n",
    "        test_counter += 1\n",
    "\n",
    "    return requirements, tests, mappings, req_counter, test_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "730ead25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_requirements = []\n",
    "all_tests = []\n",
    "all_mappings = []\n",
    "\n",
    "TARGET_PATHS = [\n",
    "    \"bookmarks\",\n",
    "    \"copy-paste\",\n",
    "    \"drag-drop\",\n",
    "    \"form-manager\",\n",
    "    \"help\",\n",
    "    \"history\",\n",
    "    \"imaging\",\n",
    "    \"oji\",\n",
    "    \"password-manager\",\n",
    "    \"printing\",\n",
    "    \"plugins\",\n",
    "    \"search\",\n",
    "    \"selection\",\n",
    "    \"sidebar\",\n",
    "    \"xpapps-gui\",\n",
    "    \"themes\",\n",
    "    \"toolbars\"\n",
    "]\n",
    "\n",
    "req_counter = 1\n",
    "test_counter = 1\n",
    "\n",
    "for path in TARGET_PATHS:\n",
    "    page_url = urljoin(PAGE_URL_TEMPLATE, path)\n",
    "    reqs, tests, maps, req_counter, test_counter = scrape_test_page(page_url, req_counter, test_counter, path)\n",
    "    \n",
    "    all_requirements.extend(reqs)\n",
    "    all_tests.extend(tests)\n",
    "    all_mappings.extend(maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b4ccc4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved: requirements.csv, tests.csv, mapping.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Write to CSV files\n",
    "with open('data/Mozilla2/RE.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['ID', 'Feature', 'Description'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_requirements)\n",
    "\n",
    "with open('data/Mozilla2/ST.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['ID', 'Purpose', 'TestSteps'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_tests)\n",
    "\n",
    "with open('data/Mozilla2/mapping.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['ReqID', 'TestID'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_mappings)\n",
    "\n",
    "print(\"Files saved: requirements.csv, tests.csv, mapping.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
